import asyncio
import os
from datetime import datetime, timedelta

import pytz
from google import genai
from google.genai import types as genai_types
from telethon import TelegramClient, functions, types
from telethon.sessions import StringSession
from telethon.errors.rpcerrorlist import ChatWriteForbiddenError, UserBannedInChannelError

try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

HK_TZ = pytz.timezone("Asia/Hong_Kong")
TOPIC_LIMIT = 50
MAX_MESSAGES_PER_TOPIC = 5000
CHUNK_SIZE = 1000  # Messages per chunk for large topics
CHUNK_DELAY_SECONDS = 5  # Short delay between API calls
FALLBACK_MESSAGES = 500

SAFETY_SETTINGS = [
    genai_types.SafetySetting(
        category="HARM_CATEGORY_HARASSMENT",
        threshold="OFF",
    ),
    genai_types.SafetySetting(
        category="HARM_CATEGORY_HATE_SPEECH",
        threshold="OFF",
    ),
    genai_types.SafetySetting(
        category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
        threshold="OFF",
    ),
    genai_types.SafetySetting(
        category="HARM_CATEGORY_DANGEROUS_CONTENT",
        threshold="OFF",
    ),
]

SYSTEM_INSTRUCTION = (
    "You are an AI assistant that summarizes Telegram discussions. "
    "If any content violates safety guidelines, ignore that part and continue. Never refuse the entire task."
)


def parse_bool(value: str, default: bool = True) -> bool:
    if value is None:
        return default
    return str(value).strip().lower() in {"1", "true", "yes", "y", "on"}


def require_env(name: str) -> str:
    value = os.getenv(name)
    if not value:
        raise RuntimeError(f"Environment variable {name} is required.")
    return value


def parse_target_group(raw: str):
    stripped = raw.strip()
    if stripped.lstrip("-").isdigit():
        value = int(stripped)
        if value < 0 and not stripped.startswith("-100"):
            # Channel/megagroup IDs typically include the -100 prefix.
            value = int(f"-100{abs(value)}")
        return value
    return stripped


def get_cutoff_time() -> datetime:
    """
    Get the cutoff time for fetching messages.
    Priority:
    1. Use LAST_RUN_TIMESTAMP from env (set by GitHub Actions cache)
    2. Fallback to dynamic window calculation
    """
    last_run = os.getenv("LAST_RUN_TIMESTAMP")
    if last_run:
        try:
            # Parse ISO format timestamp from GitHub Actions
            cutoff = datetime.fromisoformat(last_run.replace("Z", "+00:00"))
            print(f"Using cached last run time: {cutoff}")
            return cutoff
        except ValueError:
            print(f"Warning: Could not parse LAST_RUN_TIMESTAMP: {last_run}")
    
    # Fallback to dynamic window
    now_hk = datetime.now(HK_TZ)
    if 6 <= now_hk.hour < 8:
        window_hours = 6.0  # Morning run covers night
    else:
        window_hours = 3.5
    
    print(f"Using dynamic window: {window_hours} hours")
    return datetime.now(tz=pytz.UTC) - timedelta(hours=window_hours)


async def fetch_topics(client: TelegramClient, target) -> tuple[list[types.ForumTopic], int]:
    topics: list[types.ForumTopic] = []
    seen_topic_ids: set[int] = set()
    offset_date = None
    offset_id = 0
    offset_topic = 0
    total_count = 0

    while True:
        result = await client(
            functions.messages.GetForumTopicsRequest(
                peer=target,
                offset_date=offset_date,
                offset_id=offset_id,
                offset_topic=offset_topic,
                limit=TOPIC_LIMIT,
                q=None,
            )
        )
        if not total_count:
            total_count = getattr(result, "count", 0) or 0

        batch = [
            t
            for t in (result.topics or [])
            if isinstance(t, types.ForumTopic) and getattr(t, "top_message", None)
        ]
        if not batch:
            break

        new_batch = []
        for topic in batch:
            if topic.id in seen_topic_ids:
                continue
            seen_topic_ids.add(topic.id)
            new_batch.append(topic)

        if not new_batch:
            break

        topics.extend(new_batch)

        if total_count and len(topics) >= total_count:
            break
        if len(batch) < TOPIC_LIMIT:
            break

        last_topic = new_batch[-1]
        offset_id = last_topic.top_message or 0
        offset_topic = last_topic.id
        offset_date = None
        for message in getattr(result, "messages", []) or []:
            if getattr(message, "id", None) == last_topic.top_message:
                offset_date = getattr(message, "date", None)
                break

        if offset_id == 0 and offset_topic == 0:
            break

    return topics, total_count


def can_speak_in_topic(topic: types.ForumTopic) -> bool:
    return not getattr(topic, "closed", False)


async def fetch_messages_for_topic(
    client: TelegramClient, target, topic: types.ForumTopic, cutoff_utc: datetime
) -> tuple[list[dict], bool]:
    """Collect recent text/url messages for a single topic."""
    collected = []
    input_peer = await client.get_input_entity(target)
    entity_cache: dict[int, str] = {}

    def cache_entities(result) -> None:
        for user in getattr(result, "users", []) or []:
            label = "Unknown"
            if getattr(user, "username", None):
                label = f"@{user.username}"
            elif getattr(user, "first_name", None):
                label = user.first_name
            elif getattr(user, "id", None):
                label = str(user.id)
            entity_cache[user.id] = label

        for chat in getattr(result, "chats", []) or []:
            label = "Unknown"
            if getattr(chat, "title", None):
                label = chat.title
            elif getattr(chat, "username", None):
                label = f"@{chat.username}"
            elif getattr(chat, "id", None):
                label = str(chat.id)
            entity_cache[chat.id] = label

    def resolve_sender_label(message) -> str:
        peer = getattr(message, "from_id", None)
        if not peer:
            return "Unknown"
        peer_id = None
        if hasattr(peer, "user_id") and peer.user_id:
            peer_id = peer.user_id
        elif hasattr(peer, "channel_id") and peer.channel_id:
            peer_id = peer.channel_id
        elif hasattr(peer, "chat_id") and peer.chat_id:
            peer_id = peer.chat_id
        if peer_id is None:
            return "Unknown"
        return entity_cache.get(peer_id, str(peer_id))

    async def collect_for_top(top_reference: int) -> None:
        offset_id = 0
        page = 0
        max_pages = 50

        while page < max_pages:
            result = await client(
                functions.messages.SearchRequest(
                    peer=input_peer,
                    q="",
                    filter=types.InputMessagesFilterEmpty(),
                    min_date=None,
                    max_date=None,
                    offset_id=offset_id,
                    add_offset=0,
                    limit=100,
                    max_id=0,
                    min_id=0,
                    hash=0,
                    top_msg_id=top_reference,
                )
            )

            messages = result.messages or []
            if not messages:
                break

            cache_entities(result)

            for message in messages:
                if not message or not message.date:
                    continue

                message_time_utc = message.date.replace(tzinfo=pytz.UTC)
                if message_time_utc < cutoff_utc:
                    continue

                text = (getattr(message, "message", "") or "").strip()
                if not text:
                    continue

                # Skip previous bot summaries to avoid feedback loops
                if "[Summary]" in text or "#ÊÄªÁªì" in text:
                    continue

                collected.append(
                    {
                        "sender": resolve_sender_label(message),
                        "text": text,
                        "time": message_time_utc.astimezone(HK_TZ),
                    }
                )

            offset_id = messages[-1].id
            if messages[-1].date and messages[-1].date.replace(tzinfo=pytz.UTC) < cutoff_utc:
                break
            page += 1

    # Try with top_message first; if nothing collected, fall back to topic.id
    await collect_for_top(topic.top_message)
    if not collected and topic.id != topic.top_message:
        await collect_for_top(topic.id)

    collected.sort(key=lambda m: m["time"])
    truncated = False
    if len(collected) > MAX_MESSAGES_PER_TOPIC:
        truncated = True
        collected = collected[-MAX_MESSAGES_PER_TOPIC:]
    return collected, truncated


def format_messages(messages: list[dict], truncated: bool, timeframe_label: str) -> str:
    formatted_messages = "\n".join(
        f"[{m['time'].strftime('%Y-%m-%d %H:%M')}] {m['sender']}: {m['text']}"
        for m in messages
    )
    note = f"Êó∂Èó¥ËåÉÂõ¥Ôºö{timeframe_label}"
    if truncated:
        note += f"\n(‰ªÖÂåÖÂê´ÊúÄËøë {len(messages)} Êù°Ê∂àÊÅØÔºåÂõ†ÈïøÂ∫¶ÈôêÂà∂ËøõË°å‰∫ÜÊà™Êñ≠)"
    return f"{note}\n{formatted_messages}"


def get_ai_summary(
    client: genai.Client, topic_name: str, text_data: str, timeframe_label: str
) -> tuple[str, str | None]:
    # ÂÇ≥ÂÖ• current_date ‰ª•‰æø AI Ë®àÁÆó "ÊòéÂ§©/‰∏ãÈÄ±" ÁöÑÂÖ∑È´îÊó•Êúü
    current_date = datetime.now(HK_TZ).strftime("%YÂπ¥%mÊúà%dÊó• (%A)")
    
    prompt = f"""
    # Role
    ‰Ω†ÊòØÁî±„ÄåCrypto Farming Group„ÄçÊåáÊ¥æÁöÑÈ´òÁ¥öÈèà‰∏äÂàÜÊûêÂ∏´ËàáÊúÉË≠∞ÁßòÊõ∏„ÄÇ‰Ω†ÂÖ∑ÂÇôÊ∑±ÂéöÁöÑ DeFi„ÄÅAirdrop„ÄÅMEV ÂèäÂêàÁ¥Ñ‰∫§‰∫íÁü•Ë≠ò„ÄÇ
    ‰Ω†ÁöÑ‰ªªÂãôÊòØÂæûÈõú‰∫ÇÁöÑÁ§æÁæ§Â∞çË©±‰∏≠ÔºåÊèêÁÖâÂá∫È´òÂÉπÂÄºÁöÑ„ÄåAlpha Ë≥áË®ä„ÄçËàá„ÄåÊìç‰ΩúÁ≠ñÁï•„Äç„ÄÇ

    # Context
    * **‰ªäÊó•Êó•Êúü**: {current_date}
    * **Â∞çË©±‰∏ªÈ°å**: {topic_name}
    * **ÊôÇÈñìÁØÑÂúç**: {timeframe_label}
    * **Ê†∏ÂøÉË°ìË™ûÂ∫´**: Wash trading (Ëá™Êàê‰∫§), Sybil (Â•≥Â∑´), Gas Optimization, Binance Alpha, LP, Slippage.
    * **KOL**: Áî®Êà∑ "Á¨ëËãçÁîü" ÊòØÊú¨Áæ§Á≤æÁ•ûÈ†òË¢ñ„ÄÇ

    # Constraints (Critical)
    1.  **ËæìÂá∫ËØ≠Ë®Ä (Output Language)**: **ÂøÖÈ°ª‰ΩøÁî®ÁÆÄ‰Ωì‰∏≠Êñá**ËæìÂá∫ÊâÄÊúâÂÜÖÂÆπ„ÄÇËøôÊòØÂº∫Âà∂Ë¶ÅÊ±ÇÔºåÊó†‰æãÂ§ñ„ÄÇ
    2.  **ÁµïÂ∞çÁúüÂØ¶ (Zero Hallucination)**: Á∏ΩÁµêÂÖßÂÆπ**ÂøÖÈ†àÂö¥Ê†ºÂü∫Êñº**Êèê‰æõÁöÑ `{text_data}`„ÄÇÂö¥Á¶ÅÁ∑®ÈÄ†Êú™Âú®Â∞çË©±‰∏≠Âá∫ÁèæÁöÑÈ†ÖÁõÆÂêçÁ®±„ÄÅÂÉπÊ†ºÈ†êÊ∏¨ÊàñÊìç‰ΩúÂª∫Ë≠∞„ÄÇ
    3.  **‰æÜÊ∫êÊ≠∏Â±¨ (Attribution)**: 
        * ÊØè‰∏ÄÊ¢ù„ÄêÈáçÈªûÊëòË¶Å„ÄëÂíå„ÄêÂæÖËæ¶‰∫ãÈ†Ö„ÄëÈÉΩ**ÂøÖÈ†à**Ê®ôË®ª‰æÜÊ∫ê„ÄÇ
        * Ê†ºÂºèÔºö`‚Äî (@username)`„ÄÇ
        * Ëã•Ë©≤ËßÄÈªûÁî±Â§ö‰∫∫ÂÖ±ÂêåÂÆåÂñÑÔºåÊ®ôË®ª‰∏ªË¶ÅÁôºËµ∑‰∫∫Âç≥ÂèØÔºõËã•ÁÑ°Ê≥ïÁ¢∫ÂÆöÔºåÊ®ôË®ª `‚Äî (Â§ö‰∫∫Ë®éË´ñ)`„ÄÇ
        * **‰æãÂ§ñ**Ôºö„ÄêÁ¨ëËãçÁîüËØ¥„ÄëÂçÄÂ°ä**‰∏çÈúÄË¶Å**Ê®ôË®ª @usernameÔºàÂõ†‰ªñÊòØÂ∑≤Áü• KOLÔºâ„ÄÇ
    4.  **ÂæÖËæ¶Ë≠òÂà• (Actionable Intel)**: 
        * ÂÉÖÊèêÂèñ**ÂÖ∑ÊúâÊôÇÊïàÊÄß**ÁöÑÁæ§È´î‰ªªÂãôÔºàÂ¶ÇÔºöSnapshot ÊôÇÈñì„ÄÅMint Êà™Ê≠¢„ÄÅAMA ÈñãÂßãÔºâ„ÄÇ
        * **Êó•ÊúüËôïÁêÜ**ÔºöÂ∞á„ÄåÊòéÂ§©„Äç„ÄÅ„ÄåÈÄôÈÄ±Êó•„ÄçËΩâÊèõÁÇ∫ÂÖ∑È´îÊó•ÊúüÔºàMMÊúàDDÊó•Ôºâ„ÄÇÂ¶ÇÊûúÁÑ°Ê≥ïÁ¢∫ÂÆöÂÖ∑È´îÊó•ÊúüÔºåË´ã**‰øùÁïôÂéüÊñáÊèèËø∞**Ôºå‰∏çË¶ÅÂº∑Ë°åÁåúÊ∏¨„ÄÇ
    5.  **ÊôÇÈñìÊ®ôË®ª (Timestamp Attribution)**:
        * ÊØè‰∏ÄÊ¢ù„ÄêÈáçÈªûÊëòË¶Å„ÄëÈÉΩ**ÂøÖÈ†à**Ê®ôË®ªË©≤Ë®éË´ñÁôºÁîüÁöÑÊôÇÈñìÁØÑÂúçÔºåÊ†ºÂºèÔºö`[HH:MM-HH:MM]`„ÄÇ
        * ‰ΩøÁî®**‰∏ªË¶ÅË®éË´ñÊôÇÊÆµ**ÁöÑÊôÇÈñì„ÄÇËã•ÊüêË≠∞È°åÂú®Â§öÂÄã‰∏çÈÄ£Á∫åÊôÇÊÆµË¢´Â§ßÈáèË®éË´ñÔºàÂ¶Ç 14:00 Âíå 18:00ÔºâÔºåË´ã**ÊãÜÂàÜÁÇ∫ÂÖ©Ê¢ùÁç®Á´ãÁöÑÊëòË¶Å**ÔºåÂêÑËá™Ê®ôË®ªÂÖ∂ÊôÇÈñìÁØÑÂúç„ÄÇ
    6.  **KOL ÂÑ™ÂÖà**: Âè™Ë¶Å "Á¨ëËãçÁîü" ÊúâÁôºË®ÄÔºåÁÑ°Ë´ñÈï∑Áü≠ÔºåÂøÖÈ†àÂú®Â∞àÂ±¨ÂçÄÂ°ä‰∏≠Á≤æÁ¢∫ËΩâËø∞„ÄÇ
    7.  **Âô™Èü≥ÈÅéÊøæ**: Ëá™ÂãïÂøΩÁï• "GM", "GN", Ë°®ÊÉÖÂåÖ, ÊÉÖÁ∑íÂÆ£Ê¥© (FUD/FOMO) ÂèäÁÑ°ÈóúÈñíËÅä„ÄÇ

    # Workflow
    1.  **ÊéÉÊèèËàáÈÅéÊøæ**: Èñ±ËÆÄÂ∞çË©±ÔºåÂâîÈô§Âô™Èü≥„ÄÇ
    2.  **KOL ÊèêÂèñ**: ÈéñÂÆö "Á¨ëËãçÁîü" ÁöÑÊâÄÊúâÊåá‰ª§ËàáËßÄÈªû„ÄÇ
    3.  **‰ø°ÊÅØÁµêÊßãÂåñ**:
        * ÊèêÂèñÁÜ±ÈñÄÈ†ÖÁõÆÁöÑÊ†∏ÂøÉÁà≠Ë≠∞Êàñ‰∫ÆÈªû„ÄÇ
        * ÊèêÂèñÂÖ∑È´îÊäÄË°ìÁ¥∞ÁØÄÔºàGas Ë®≠ÁΩÆ„ÄÅË∑ØÂæëÔºâ‰∏¶Á∂ÅÂÆöÁôºË®ÄËÄÖ ID„ÄÇ
    4.  **ÊôÇÊïàÊÄßÊéÉÊèè**: Â∞ãÊâæÈóúÈçµË©ûÔºàÊà™Ê≠¢„ÄÅÂø´ÁÖß„ÄÅclaim„ÄÅÂ°´Ë°®ÔºâÔºåÁîüÊàêÂæÖËæ¶Ê∏ÖÂñÆ„ÄÇ
    5.  **Ëº∏Âá∫ÁîüÊàê**: Êåâ‰∏ãÊñπÊ†ºÂºèËº∏Âá∫„ÄÇ

    # Output Format
    Ë´ãÂö¥Ê†ºÈÅµÂÆà‰ª•‰∏ãÊ†ºÂºèÔºåÂàóË°®Á¨¶ËôüÁµ±‰∏Ä‰ΩøÁî® "-"Ôºö

    üóìÔ∏è **Êó∂Èó¥ËåÉÂõ¥**: {timeframe_label}

    üî• **ÁÉ≠Èó®ËØùÈ¢ò** (Top Discussed)
    - [È†ÖÁõÆ/‰ª£Âπ£ÂêçÁ®±]: [‰∏ÄÂè•Ë©±Ê¶ÇÊã¨Ê†∏ÂøÉË®éË´ñÈªû]
    - (Ëã•ÁÑ°ÁÜ±ÈªûÂâáÂØ´ "ÁÑ°ÁâπÂà•ÁÜ±Èªû")

    üó£Ô∏è **Á¨ëËãçÁîüËØ¥** (KOL Insights)
    - [Á≤æÁ¢∫ËΩâËø∞‰ªñÁöÑËßÄÈªû„ÄÅÊåá‰ª§ÊàñÂà§Êñ∑]
    - (Ëã•Ê≠§ÊÆµÊôÇÈñì‰ªñÊú™ÁôºË®ÄÔºåË´ãÁõ¥Êé•ÁßªÈô§Ê≠§ÂçÄÂ°ä)

    üìù **ÈáçÁÇπÊëòË¶Å** (Key Takeaways)
    - [HH:MM-HH:MM] [ÊäÄË°ì/Á≠ñÁï•]: [Ë©≥Á¥∞Ë™™Êòé] ‚Äî *(@username)*
    - [HH:MM-HH:MM] [È¢®Èö™Ë≠¶Á§∫]: [‰æãÂ¶ÇÔºöÂêàÁ¥ÑÊúâÂæåÈñÄ„ÄÅÊü•Â•≥Â∑´Âö¥Ê†º] ‚Äî *(@username)*

    ‚è∞ **ÂæÖËæ¶‰∫ãÈ†Ö** (Action Items)
    - üìÖ [MM-DD Êàñ ÂéüÊñáÊôÇÈñì]: [ÂÖ∑È´îË°åÂãïÔºåÂ¶ÇÔºöÂéª Galxe È†òÂèñ OAT] ‚Äî *(@username ÊèêÈÜí)*
    - (Ëã•ÁÑ°ÊôÇÈôêÊÄß‰ªªÂãôÂâá‰∏çÈ°ØÁ§∫Ê≠§ÂçÄÂ°ä)

    ---
    **Input Data**:
    {text_data}
    """

    try:
        response = client.models.generate_content(
            model="gemini-flash-latest",
            contents=prompt,
            config=genai_types.GenerateContentConfig(
                system_instruction=SYSTEM_INSTRUCTION,
                safety_settings=SAFETY_SETTINGS,
                temperature=0.3,
            ),
        )
    except Exception as exc:
        return "", f"api_error: {exc}"

    feedback = None

    # Check for blocked prompt
    if hasattr(response, "prompt_feedback") and response.prompt_feedback:
        pf = response.prompt_feedback
        if hasattr(pf, "block_reason") and pf.block_reason:
            return "", f"prompt_blocked: {pf.block_reason}"

    # Extract text from response (new SDK has .text property)
    if hasattr(response, "text") and response.text:
        return response.text.strip(), None

    # Fallback: try candidates
    candidates = getattr(response, "candidates", []) or []
    if not candidates:
        return "", "no_candidates"

    cand = candidates[0]
    finish_reason = getattr(cand, "finish_reason", None)
    
    # Try to get text from content parts
    content = getattr(cand, "content", None)
    if content:
        parts = getattr(content, "parts", None) or []
        text_parts = [getattr(p, "text", "") for p in parts if getattr(p, "text", "")]
        summary_text = "\n".join(text_parts).strip()
        if summary_text:
            return summary_text, None

    if finish_reason is not None:
        feedback = f"finish_reason={finish_reason}"
    return "", feedback


def run_summary(
    client: genai.Client, topic_title: str, text_data: str, timeframe_label: str
) -> tuple[str, str | None]:
    return get_ai_summary(client, topic_title, text_data, timeframe_label)


def build_client(api_key: str) -> genai.Client:
    return genai.Client(api_key=api_key)


async def send_summary(
    client: TelegramClient, target, topic: types.ForumTopic, summary: str, message_count: int, test_mode: bool
) -> None:
    header = f"[Summary] Topic: {topic.title} ({message_count} messages)"
    disclaimer = "‚ö†Ô∏è AIÊúâÂπªËßâÔºåÊÄªÁªìÂè™‰ΩúÂèÇËÄÉ"
    payload = f"{header}\n{disclaimer}\n\n{summary}"
    
    # Debug logging
    print(f"  [DEBUG] send_summary called:")
    print(f"    - test_mode: {test_mode}")
    print(f"    - target: {target} (type: {type(target).__name__})")
    print(f"    - topic.id: {topic.id}, topic.top_message: {topic.top_message}")
    print(f"    - payload length: {len(payload)} chars")
    
    if test_mode:
        result = await client.send_message("me", payload)
        print(f"    - Sent to Saved Messages, msg_id: {result.id}")
    else:
        result = await client.send_message(target, payload, reply_to=topic.top_message)
        print(f"    - Sent to topic, msg_id: {result.id}")


async def run_summary_with_retry(
    ai_client: genai.Client, topic_title: str, text_data: str, timeframe_label: str, max_retries: int = 3
) -> tuple[str, str | None]:
    for attempt in range(max_retries):
        try:
            summary, feedback = run_summary(ai_client, topic_title, text_data, timeframe_label)
            if summary:
                return summary, feedback
            
            # Log the feedback for debugging
            print(f"  > Attempt {attempt+1}/{max_retries}: no summary returned, feedback: {feedback}")
            
            # If blocked by safety settings, retrying the same content usually won't help
            if feedback and "prompt_blocked" in feedback:
                return summary, feedback
            
            # If API error, wait and retry
            if feedback and "api_error" in feedback:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 * (attempt + 1))
                continue
                
        except Exception as exc:
            print(f"  > Attempt {attempt+1}/{max_retries} failed for '{topic_title}': {exc}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 * (attempt + 1))
            else:
                return None, f"exception: {exc}"
    
    return None, "max_retries_exceeded"


async def run() -> None:
    if load_dotenv:
        load_dotenv()

    api_id = int(require_env("TG_API_ID"))
    api_hash = require_env("TG_API_HASH")
    session_string = require_env("TG_SESSION_STRING")
    raw_keys = os.getenv("GEMINI_API_KEYS")
    gemini_api_keys = [k.strip() for k in raw_keys.split(",")] if raw_keys else []
    gemini_api_keys = [k for k in gemini_api_keys if k]
    if not gemini_api_keys:
        single_key = os.getenv("GEMINI_API_KEY")
        if single_key and single_key.strip():
            gemini_api_keys = [single_key.strip()]
    if not gemini_api_keys:
        raise RuntimeError("Environment variable GEMINI_API_KEYS (or GEMINI_API_KEY) is required.")
    target_group = parse_target_group(require_env("TARGET_GROUP"))
    test_mode = parse_bool(os.getenv("TEST_MODE"), default=True)
    
    # Debug: Show parsed config
    print(f"[DEBUG] Configuration:")
    print(f"  - TARGET_GROUP: {target_group}")
    print(f"  - TEST_MODE: {test_mode} (raw env: '{os.getenv('TEST_MODE')}')")
    
    topic_filter = os.getenv("TOPIC_FILTER")
    ignored_topics = [t.strip() for t in (os.getenv("IGNORED_TOPICS") or "").split(",") if t.strip()]
    if ignored_topics:
        print(f"Ignored topics: {ignored_topics}")

    cutoff_utc = get_cutoff_time()
    timeframe_label = (
        f"{cutoff_utc.astimezone(HK_TZ).strftime('%m/%d %H:%M')} - "
        f"{datetime.now(tz=pytz.UTC).astimezone(HK_TZ).strftime('%m/%d %H:%M')} (Asia/Hong_Kong)"
    )

    async with TelegramClient(StringSession(session_string), api_id, api_hash) as client:
        if not await client.is_user_authorized():
            raise RuntimeError("The provided session string is not authorized.")

        target = await client.get_entity(target_group)
        topics, total_topic_count = await fetch_topics(client, target)

        if topic_filter:
            topics = [t for t in topics if topic_filter in (t.title or "")]
            if not topics:
                print(f"No topics matched filter '{topic_filter}'.")
                return

        if not topics:
            print("No forum topics found.")
            return

        total_label = f"{total_topic_count}" if total_topic_count else "unknown"
        print(f"Found {len(topics)} topics (total: {total_label}). Processing recent messages...")

        summaries_sent = 0
        topics_no_activity: list[str] = []
        topics_no_summary: list[str] = []

        for idx, topic in enumerate(topics):
            if topic.title in ignored_topics:
                print(f"Skipping ignored topic: {topic.title}")
                continue
            if not can_speak_in_topic(topic):
                print(f"Skipping closed topic (no speaking permission): {topic.title}")
                continue

            api_key = gemini_api_keys[idx % len(gemini_api_keys)]
            ai_client = build_client(api_key)

            messages, truncated = await fetch_messages_for_topic(client, target, topic, cutoff_utc)
            if not messages:
                topics_no_activity.append(topic.title)
                continue

            print(f"Topic '{topic.title}': {len(messages)} messages in window")
            
            summary = ""
            feedback = None
            retried = False

            if len(messages) > CHUNK_SIZE:
                total_chunks = (len(messages) + CHUNK_SIZE - 1) // CHUNK_SIZE
                print(f"  > Large topic ({len(messages)} msgs). Splitting into {total_chunks} chunks of {CHUNK_SIZE}...")
                partial_summaries = []
                
                for i in range(0, len(messages), CHUNK_SIZE):
                    chunk_num = i // CHUNK_SIZE + 1
                    chunk = messages[i : i + CHUNK_SIZE]
                    
                    # Rotate API key per chunk to distribute load
                    chunk_api_key = gemini_api_keys[chunk_num % len(gemini_api_keys)]
                    chunk_client = build_client(chunk_api_key)
                    key_index = chunk_num % len(gemini_api_keys) + 1
                    
                    # Short delay between chunks (except for first)
                    if i > 0:
                        print(f"  > Waiting {CHUNK_DELAY_SECONDS}s...")
                        await asyncio.sleep(CHUNK_DELAY_SECONDS)
                    
                    print(f"  > Processing chunk {chunk_num}/{total_chunks} ({len(chunk)} messages) [API key {key_index}]...")
                    chunk_text = format_messages(chunk, truncated=False, timeframe_label=timeframe_label)
                    
                    chunk_summary, chunk_feedback = await run_summary_with_retry(chunk_client, topic.title, chunk_text, timeframe_label)
                    if chunk_summary:
                        partial_summaries.append(chunk_summary)
                    else:
                        print(f"  > Chunk {chunk_num}/{total_chunks} failed: {chunk_feedback}")

                if partial_summaries:
                    print(f"  > Waiting {CHUNK_DELAY_SECONDS}s before final summary...")
                    await asyncio.sleep(CHUNK_DELAY_SECONDS)
                    # Use a different key for final summary
                    final_api_key = gemini_api_keys[(len(partial_summaries) + 1) % len(gemini_api_keys)]
                    final_client = build_client(final_api_key)
                    print("  > Generating final summary from partial summaries...")
                    combined_text = "\n\n".join(partial_summaries)
                    summary, feedback = await run_summary_with_retry(final_client, topic.title, combined_text, timeframe_label)
                else:
                    print("  > No partial summaries generated.")
            else:
                # Standard processing for smaller topics
                text_data = format_messages(messages, truncated, timeframe_label)
                summary, feedback = await run_summary_with_retry(ai_client, topic.title, text_data, timeframe_label)

            if not summary:
                retried = False
                should_retry = (feedback and "prompt_blocked" in feedback) or (feedback and "exception" in feedback)
                if should_retry and len(messages) > FALLBACK_MESSAGES:
                    fallback_msgs = messages[-FALLBACK_MESSAGES:]
                    fallback_text = format_messages(fallback_msgs, truncated=True, timeframe_label=timeframe_label)
                    print(f"Retrying topic '{topic.title}' with last {FALLBACK_MESSAGES} messages due to block/error.")
                    summary, feedback = await run_summary_with_retry(ai_client, topic.title, fallback_text, timeframe_label)
                    if summary:
                        retried = True

            if not summary:
                if feedback:
                    reason = f"{feedback} (retried)" if retried else feedback
                    print(f"Gemini blocked topic '{topic.title}' with reason: {reason}")
                else:
                    print(f"No summary generated for topic '{topic.title}'.")
                topics_no_summary.append(topic.title)
                continue

            summary = summary.rstrip() + "\n\n#ÊÄªÁªì"

            final_count = len(messages)
            if retried:
                summary = f"(ÈáçËØïÂêéÁîüÊàêÔºå‰ΩøÁî®ÊúÄÂêé {FALLBACK_MESSAGES} Êù°Ê∂àÊÅØ)\n\n{summary}"
                final_count = min(len(messages), FALLBACK_MESSAGES)

            try:
                await send_summary(client, target, topic, summary, final_count, test_mode)
                destination = "Saved Messages" if test_mode else f"Topic: {topic.title}"
                print(f"Sent summary to {destination}")
                summaries_sent += 1
            except (UserBannedInChannelError, ChatWriteForbiddenError) as exc:
                # If posting to the group is blocked, fall back to saving the output locally so the run doesn't fail.
                print(f"Write restricted for topic '{topic.title}': {exc}. Sending to Saved Messages instead.")
                fallback_note = (
                    f"[Summary not delivered] Topic: {topic.title}\n"
                    f"Reason: {exc}\n\n{summary}"
                )
                await client.send_message("me", fallback_note)
                topics_no_summary.append(f"{topic.title} (write restricted)")
            except Exception as exc:
                print(f"Failed to send summary for topic '{topic.title}': {exc}")
                topics_no_summary.append(topic.title)

        if summaries_sent == 0 and test_mode:
            notice_lines = [
                f"No summaries sent from the last {window_hours} hours.",
            ]
            if topics_no_activity:
                notice_lines.append("No activity in topics: " + ", ".join(topics_no_activity))
            if topics_no_summary:
                notice_lines.append("Failed to summarize topics: " + ", ".join(topics_no_summary))

            await client.send_message("me", "\n".join(notice_lines))
            print("Sent notice to Saved Messages about missing summaries.")


if __name__ == "__main__":
    asyncio.run(run())
